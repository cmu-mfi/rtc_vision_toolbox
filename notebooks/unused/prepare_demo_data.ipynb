{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import glob\n",
    "# import re\n",
    "# import open3d as o3d\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# def plot_multi_np(plist):\n",
    "#     \"\"\"\n",
    "#     Args: plist, list of numpy arrays of shape, (1,num_points,3)\n",
    "#     \"\"\"\n",
    "#     colors = [\n",
    "#         '#1f77b4',  # muted blue\n",
    "#         '#ff7f0e',  # safety orange\n",
    "#         '#2ca02c',  # cooked asparagus green\n",
    "#         '#d62728',  # brick red\n",
    "#         '#9467bd',  # muted purple\n",
    "#         '#e377c2',  # raspberry yogurt pink\n",
    "#         '#8c564b',  # chestnut brown\n",
    "#         '#7f7f7f',  # middle gray\n",
    "#         '#bcbd22',  # curry yellow-green\n",
    "#         '#17becf',  # blue-teal\n",
    "#         '#1f77b4',  # muted blue\n",
    "#         '#ff7f0e',  # safety orange\n",
    "#         '#2ca02c',  # cooked asparagus green\n",
    "#         '#d62728',  # brick red\n",
    "#         '#9467bd',  # muted purple\n",
    "#         '#e377c2',  # raspberry yogurt pink\n",
    "#         '#8c564b',  # chestnut brown\n",
    "#         '#7f7f7f',  # middle gray\n",
    "#         '#bcbd22',  # curry yellow-green\n",
    "#         '#17becf',  # blue-teal\n",
    "#         '#1f77b4',  # muted blue\n",
    "#         '#ff7f0e',  # safety orange\n",
    "#         '#2ca02c',  # cooked asparagus green\n",
    "#         '#d62728',  # brick red\n",
    "#         '#9467bd',  # muted purple\n",
    "#         '#e377c2',  # raspberry yogurt pink\n",
    "#         '#8c564b',  # chestnut brown\n",
    "#         '#7f7f7f',  # middle gray\n",
    "#         '#bcbd22',  # curry yellow-green\n",
    "#         '#17becf',  # blue-teal\n",
    "#     ]\n",
    "#     skip = 1\n",
    "#     go_data = []\n",
    "#     for i in range(len(plist)):\n",
    "#         p_dp = plist[i]\n",
    "#         plot = go.Scatter3d(x=p_dp[::skip,0], y=p_dp[::skip,1], z=p_dp[::skip,2], \n",
    "#                      mode='markers', marker=dict(size=2, color=colors[i],\n",
    "#                      symbol='circle'))\n",
    "#         go_data.append(plot)\n",
    " \n",
    "#     layout = go.Layout(\n",
    "#         scene=dict(\n",
    "#             aspectmode='data',\n",
    "#         ),\n",
    "#         height=1200,\n",
    "#         width=1200,\n",
    "#     )\n",
    "\n",
    "#     fig = go.Figure(data=go_data, layout=layout)\n",
    "#     fig.show()\n",
    "#     return fig\n",
    "\n",
    "# pcd_files = glob.glob('/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/place_demo_pcds/wp-grasp-place-0/2024-04-25-17-34-34/*.npz')\n",
    "\n",
    "# cameras = {}\n",
    "# for filepath in pcd_files:\n",
    "#     filename = filepath.split('/')[-1]\n",
    "#     match = re.match(r\"([a-zA-Z0-9]+)[_|-](.+).npz\", filename)\n",
    "#     camera = match.group(1)\n",
    "#     if camera not in cameras.keys():\n",
    "#         cameras[camera] = {\n",
    "#             'pcds': {}\n",
    "#         }\n",
    "    \n",
    "#     pcd = np.load(filepath)['arr_0']\n",
    "#     cameras[camera]['pcds'][match.group(2)] = pcd\n",
    "    \n",
    "# # print(f'cameras: \\n{cameras}')\n",
    "\n",
    "# transform_files = glob.glob(f'/home/mfi/repos/rtc_vision_toolbox/data/calibration_data/T_*.npy')\n",
    "# transforms = []\n",
    "# for filepath in transform_files:\n",
    "#     print(f'filename: {filepath.split(\"/\")[-1]}')\n",
    "#     transform = np.load(filepath, allow_pickle=True)\n",
    "#     transforms.append(transform)\n",
    "\n",
    "\n",
    "\n",
    "# CAM2BASE_TF = {\n",
    "#     'CL8FC3100RL': transforms[2], # this is T_base2cam0.npy\n",
    "#     'CL8FC3100W3': transforms[1], # this is T_base2cam1.npy\n",
    "#     'CL8FC3100NM': transforms[0], # this is T_base2cam2.npy\n",
    "# }\n",
    "\n",
    "# # Closest is \n",
    "# #   CL8FC3100RL : T_base2cam0.npy\n",
    "# #   CL8FC3100NM : T_base2cam2.npy\n",
    "\n",
    "# out_of_way_pcds_world = []\n",
    "# out_of_way_pcds_world_tf = []\n",
    "# for camera, data in cameras.items():\n",
    "#     # if camera in ['CL8FC3100W3', 'CL8FC3100RL']:\n",
    "#     #     continue\n",
    "#     if 'out_of_way' in data['pcds'].keys():\n",
    "#         pcd = data['pcds']['out_of_way'] / 1000\n",
    "        \n",
    "#         print(f'pcd: {pcd.shape}')\n",
    "#         print(f'CAM2BASE_TF: {CAM2BASE_TF[camera].shape}')\n",
    "        \n",
    "#         o3d_pcd = o3d.geometry.PointCloud()\n",
    "#         o3d_pcd.points = o3d.utility.Vector3dVector(pcd)\n",
    "        \n",
    "#         o3d_pcd = o3d_pcd.uniform_down_sample(100)\n",
    "#         np_pcd = np.asarray(o3d_pcd.points)\n",
    "#         print(f'np_pcd: {np_pcd.shape}')\n",
    "        \n",
    "#         np_pcd_h = np.hstack((np_pcd, np.ones((np_pcd.shape[0], 1))))\n",
    "#         print(f'np_pcd_h: {np_pcd_h.shape}')\n",
    "        \n",
    "#         # np_pcd_h = np.matmul(np.array([\n",
    "#         #     [0, 0, 1, 0],\n",
    "#         #     [1, 0, 0, 0],\n",
    "#         #     [0, 1, 0, 0],\n",
    "#         #     [0, 0, 0, 1]\n",
    "#         # ]), np_pcd_h.T).T\n",
    "        \n",
    "#         np_pcd_tf = np.matmul(CAM2BASE_TF[camera], np_pcd_h.T).T[:, :3]\n",
    "#         print(f'np_pcd_tf: {np_pcd_tf.shape}')\n",
    "\n",
    "#         # o3d_pcd = o3d_pcd.transform(np.linalg.inv(CAM2BASE_TF[camera]))\n",
    "        \n",
    "#         # np_pcd = np.asarray(o3d_pcd.points)\n",
    "#         # print(f'np_pcd: {np_pcd.shape}')\n",
    "#         out_of_way_pcds_world.append(np_pcd)\n",
    "#         out_of_way_pcds_world_tf.append(np_pcd_tf)\n",
    "\n",
    "# plot_multi_np(out_of_way_pcds_world_tf + out_of_way_pcds_world)\n",
    "\n",
    "# in_hand_pcd = np.load('/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/place_demo_pcds/2024-04-05-18-42-58/CL8FC3100NM-0_in_hand.npz', allow_pickle=True)['arr_0']\n",
    "# print(f'in_hand_pcd: {in_hand_pcd.shape}')\n",
    "\n",
    "# in_hand_pcd_filtered = in_hand_pcd[(in_hand_pcd[:,0] > -50) & (in_hand_pcd[:,0] < 50) & (in_hand_pcd[:,1] > -50) & (in_hand_pcd[:,1] < 50) & (in_hand_pcd[:,2] > 0) & (in_hand_pcd[:,2] < 450)] / 1000\n",
    "\n",
    "# in_hand_pose = np.load('/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/place_demo_preprogrammed_poses/close_up_pose.npz', allow_pickle=True)['arr_0']\n",
    "# print(f'in_hand_pose: {in_hand_pose}')\n",
    "\n",
    "# place_pose = np.load('/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/place_demo_preprogrammed_poses/placement_pose.npz', allow_pickle=True)['arr_0']\n",
    "# print(f'place_pose: {place_pose}')\n",
    "\n",
    "# in_hand_pcd_h = np.hstack((in_hand_pcd_filtered, np.ones((in_hand_pcd_filtered.shape[0], 1))))\n",
    "\n",
    "\n",
    "# # Convert coordinate system from camera (z forward, y down, x right) to world (z up, x forward, y left)\n",
    "# in_hand_pcd_h = np.matmul(np.array([\n",
    "#     [0, 0, 1, 0],\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 1, 0, 0],\n",
    "#     [0, 0, 0, 1]\n",
    "# ]), in_hand_pcd_h.T).T\n",
    "\n",
    "# print(f'in_hand_pcd_h: {in_hand_pcd_h.shape}')\n",
    "\n",
    "# in_hand_pcd_baseframe = np.matmul(np.linalg.inv(CAM2BASE_TF['CL8FC3100NM']), in_hand_pcd_h.T).T\n",
    "\n",
    "\n",
    "# base_pcd = np.matmul(np.linalg.inv(in_hand_pose), in_hand_pcd_baseframe.T).T\n",
    "# place_pcd = np.matmul(place_pose, base_pcd.T).T\n",
    "# plot_multi_np(out_of_way_pcds_world_tf + out_of_way_pcds_world + [in_hand_pcd_filtered, place_pcd])\n",
    "\n",
    "# new_o3d_in_hand_pcd_tf = o3d_in_hand_pcd.transform(np.linalg.inv(in_hand_pose))\n",
    "# new_new_o3d_in_hand_pcd_tf = new_o3d_in_hand_pcd_tf.transform(place_pose)\n",
    "\n",
    "# np_in_hand_pcd_tf = np.asarray(new_new_o3d_in_hand_pcd_tf.points)\n",
    "\n",
    "# plot_multi_np([np_in_hand_pcd_tf, np_in_hand_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import open3d as o3d\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def plot_multi_np(plist):\n",
    "    \"\"\"\n",
    "    Args: plist, list of numpy arrays of shape, (1,num_points,3)\n",
    "    \"\"\"\n",
    "    colors = [\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#d62728',  # brick red\n",
    "        '#9467bd',  # muted purple\n",
    "        '#e377c2',  # raspberry yogurt pink\n",
    "        '#8c564b',  # chestnut brown\n",
    "        '#7f7f7f',  # middle gray\n",
    "        '#bcbd22',  # curry yellow-green\n",
    "        '#17becf',  # blue-teal\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#d62728',  # brick red\n",
    "        '#9467bd',  # muted purple\n",
    "        '#e377c2',  # raspberry yogurt pink\n",
    "        '#8c564b',  # chestnut brown\n",
    "        '#7f7f7f',  # middle gray\n",
    "        '#bcbd22',  # curry yellow-green\n",
    "        '#17becf',  # blue-teal\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#d62728',  # brick red\n",
    "        '#9467bd',  # muted purple\n",
    "        '#e377c2',  # raspberry yogurt pink\n",
    "        '#8c564b',  # chestnut brown\n",
    "        '#7f7f7f',  # middle gray\n",
    "        '#bcbd22',  # curry yellow-green\n",
    "        '#17becf',  # blue-teal\n",
    "    ]\n",
    "    skip = 1\n",
    "    go_data = []\n",
    "    for i in range(len(plist)):\n",
    "        p_dp = plist[i]\n",
    "        plot = go.Scatter3d(x=p_dp[::skip,0], y=p_dp[::skip,1], z=p_dp[::skip,2], \n",
    "                     mode='markers', marker=dict(size=2, color=colors[i],\n",
    "                     symbol='circle'))\n",
    "        go_data.append(plot)\n",
    " \n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            aspectmode='data',\n",
    "        ),\n",
    "        height=1200,\n",
    "        width=1200,\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=go_data, layout=layout)\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "def setup_transforms(transform_dir: str) -> Tuple[Dict[str, np.ndarray], List[np.ndarray]]:\n",
    "    transform_files = glob.glob(f'{transform_dir}/T_*.npy')\n",
    "    transforms = []\n",
    "    for filepath in transform_files:\n",
    "        print(f'filename: {filepath.split(\"/\")[-1]}')\n",
    "        transform = np.load(filepath, allow_pickle=True)\n",
    "        transforms.append(transform)\n",
    "\n",
    "    CAM2BASE_TF = {\n",
    "        'CL8FC3100RL': transforms[2], # this is T_base2cam0.npy\n",
    "        'CL8FC3100W3': transforms[1], # this is T_base2cam1.npy\n",
    "        'CL8FC3100NM': transforms[0], # this is T_base2cam2.npy\n",
    "    }\n",
    "\n",
    "    T = [\n",
    "        np.array(CAM2BASE_TF['CL8FC3100RL']),\n",
    "        np.array(CAM2BASE_TF['CL8FC3100W3']),\n",
    "        np.array(CAM2BASE_TF['CL8FC3100NM'])\n",
    "    ]\n",
    "\n",
    "    origin = np.array([[1, 0, 0, 0],\n",
    "                        [0, 1, 0, 0],\n",
    "                        [0, 0, 1, 0],\n",
    "                        [0, 0, 0, 1]])\n",
    "    T.append(origin)\n",
    "    \n",
    "    return T, CAM2BASE_TF\n",
    "\n",
    "transforms_dir = '/home/mfi/repos/rtc_vision_toolbox/data/calibration_data'\n",
    "T, CAM2BASE_TF = setup_transforms(transforms_dir)\n",
    "# print(f'T: {T} | CAM2BASE_TF: {CAM2BASE_TF}')\n",
    "\n",
    "\n",
    "def load_demo_pcds(demo_dir: str) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "    pcd_files = glob.glob(f'{demo_dir}/*.npz')\n",
    "    pcd_files.sort()\n",
    "\n",
    "    cameras = {}\n",
    "    for filepath in pcd_files:\n",
    "        filename = filepath.split('/')[-1]\n",
    "        match = re.match(r\"([a-zA-Z0-9]+)[_|-]([a-zA-Z_]+).*.npz\", filename)\n",
    "        camera = match.group(1)\n",
    "        if camera not in cameras.keys():\n",
    "            cameras[camera] = {\n",
    "                'pcds': {}\n",
    "            }\n",
    "\n",
    "        pcd = np.load(filepath)['arr_0']\n",
    "        \n",
    "        pcd_name = match.group(2)\n",
    "        if pcd_name[-1] == '_':\n",
    "            pcd_name = pcd_name[:-1]\n",
    "        \n",
    "        if pcd_name not in cameras[camera]['pcds'].keys():\n",
    "            cameras[camera]['pcds'][pcd_name] = [pcd]\n",
    "        else:\n",
    "            cameras[camera]['pcds'][pcd_name].append(pcd)\n",
    "\n",
    "    return cameras\n",
    "\n",
    "demo_dir = '/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/place_demo_pcds/wp-grasp-place-0/2024-04-25-17-34-34'\n",
    "camera_pcds = load_demo_pcds(demo_dir)\n",
    "for camera, data in camera_pcds.items():\n",
    "    print(f'camera: {camera} | keys: {data[\"pcds\"].keys()}')\n",
    "    for key, value in data['pcds'].items():\n",
    "        print(f'\\tkey: {key} | value: {len(value)}')\n",
    "    \n",
    "    \n",
    "def load_demo_poses(demo_poses_dir: str) -> Dict[str, np.ndarray]:\n",
    "    pose_files = glob.glob(f'{demo_poses_dir}/*.npz')\n",
    "    pose_files.sort()\n",
    "\n",
    "    poses = {}\n",
    "    for filepath in pose_files:\n",
    "        filename = filepath.split('/')[-1]\n",
    "        match = re.match(r\"([a-z_A-Z_0-9]+_pose)([_0-9]+)?.npz\", filename)\n",
    "        pose = np.load(filepath, allow_pickle=True)['arr_0']\n",
    "        if match.group(1) not in poses.keys():\n",
    "            poses[match.group(1)] = [pose]\n",
    "        else:\n",
    "            poses[match.group(1)].append(pose)\n",
    "\n",
    "    return poses\n",
    "\n",
    "demo_poses_dir = '/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/preprogrammed_poses/wp-grasp-place-0'\n",
    "demo_poses = load_demo_poses(demo_poses_dir)\n",
    "for key, value in demo_poses.items():\n",
    "    print(f'key: {key} | value: {len(value)}')\n",
    "\n",
    "\n",
    "def filter_pcd(pcd: np.ndarray, x_range: Tuple[int, int], y_range: Tuple[int, int], z_range: Tuple[int, int]) -> np.ndarray:\n",
    "    return pcd[(pcd[:,0] > x_range[0]) & (pcd[:,0] < x_range[1]) & \n",
    "               (pcd[:,1] > y_range[0]) & (pcd[:,1] < y_range[1]) & \n",
    "               (pcd[:,2] > z_range[0]) & (pcd[:,2] < z_range[1])]\n",
    "\n",
    "PCD_2_POSE_IDX_MAP = {\n",
    "    0: 3,\n",
    "    1: 4,\n",
    "    2: 2,\n",
    "    3: 1,\n",
    "    4: 0\n",
    "}\n",
    "\n",
    "def get_taxpose_place_data(\n",
    "    camera_pcds: Dict[str, Dict[str, np.ndarray]], \n",
    "    demo_poses: Dict[str, np.ndarray], \n",
    "    T: List[np.ndarray],\n",
    "    CAM2BASE_TF: Dict[str, np.ndarray],\n",
    "    board_t: Tuple[float, float, float] = [0.24, -0.35, 0.15],\n",
    "    board_t_jitter: Tuple[float, float, float] = [0.0, 0.0, 0.0],\n",
    "    num_samples: int = 1,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \n",
    "    # Get the point cloud of the scene\n",
    "    out_of_way_pcds_world_tf = {}\n",
    "    for camera, data in camera_pcds.items():\n",
    "        if 'out_of_way' in data['pcds'].keys():\n",
    "            # TODO: For now assuming 1 scene pcd per camera\n",
    "            pcd = data['pcds']['out_of_way'][0] / 1000\n",
    "\n",
    "            o3d_pcd = o3d.geometry.PointCloud()\n",
    "            o3d_pcd.points = o3d.utility.Vector3dVector(pcd)\n",
    "\n",
    "            # o3d_pcd = o3d_pcd.voxel_down_sample(0.001)\n",
    "            np_pcd = np.asarray(o3d_pcd.points)\n",
    "\n",
    "            np_pcd_h = np.hstack((np_pcd, np.ones((np_pcd.shape[0], 1))))\n",
    "\n",
    "            np_pcd_tf = np.matmul(CAM2BASE_TF[camera], np_pcd_h.T).T[:, :3]\n",
    "\n",
    "            # TODO: For now assuming 1 scene pcd per camera\n",
    "            out_of_way_pcds_world_tf[camera] = [np_pcd_tf]\n",
    "            \n",
    "    # TODO: Add merging here\n",
    "    # For now just take CL8FC3100RL\n",
    "    SCENE_CAMERA = 'CL8FC3100RL'\n",
    "    scene_pcd = out_of_way_pcds_world_tf[SCENE_CAMERA][0]\n",
    "    \n",
    "    INHAND_CAMERA = 'CL8FC3100NM'\n",
    "    \n",
    "    full_samples = []\n",
    "    for i in range(num_samples):\n",
    "        for inhand_idx in range(len(camera_pcds[INHAND_CAMERA]['pcds']['in_hand'])):\n",
    "            # Filter the scene point cloud\n",
    "            board_jitter = [\n",
    "                np.random.uniform(-board_t_jitter[0], board_t_jitter[0]),\n",
    "                np.random.uniform(-board_t_jitter[1], board_t_jitter[1]),\n",
    "                np.random.uniform(-board_t_jitter[2], board_t_jitter[2])\n",
    "            ]\n",
    "            \n",
    "            x_range = (board_t[0] + board_jitter[0] - 0.2, board_t[0] + board_jitter[0] + 0.2)\n",
    "            y_range = (board_t[1] + board_jitter[1] - 0.2, board_t[1] + board_jitter[1] + 0.2)\n",
    "            z_range = (board_t[2] + board_jitter[2] - 0.05, board_t[2] + board_jitter[2] + 0.15)\n",
    "            print(f'scene: x_range: {x_range} | y_range: {y_range} | z_range: {z_range}')\n",
    "            scene_pcd_filtered = filter_pcd(scene_pcd, x_range, y_range, z_range)\n",
    "            \n",
    "            # TODO: Remove these hardcodes\n",
    "            # Get the point cloud of the object in hand\n",
    "            # TODO: For now assuming 1 in hand pcd\n",
    "            assert 'in_hand' in camera_pcds[INHAND_CAMERA]['pcds'].keys()\n",
    "            in_hand_pcd = camera_pcds[INHAND_CAMERA]['pcds']['in_hand'][inhand_idx] / 1000\n",
    "            \n",
    "            # Transform to robot base frame\n",
    "            cam2base_tf = CAM2BASE_TF[INHAND_CAMERA]\n",
    "            in_hand_pcd_h = np.hstack((in_hand_pcd, np.ones((in_hand_pcd.shape[0], 1))))\n",
    "            in_hand_pcd_baseframe = np.matmul(cam2base_tf, in_hand_pcd_h.T).T[:, :3]\n",
    "            \n",
    "            # Filter the point cloud\n",
    "            # TODO: For now assuming 1 in hand pcd\n",
    "            # TODO: Fix this: Need to do this mapping because poses were loaded out of order during demo collection\n",
    "            in_hand_pose = demo_poses['inhand_close_up_pose'][PCD_2_POSE_IDX_MAP[inhand_idx]]\n",
    "            \n",
    "            in_hand_t = in_hand_pose[:3, 3].copy()\n",
    "            eef_to_gripper_offset = np.array([0, -0.15, 0])\n",
    "            in_hand_t += eef_to_gripper_offset\n",
    "            x_range = (in_hand_t[0] - 0.15, in_hand_t[0] + 0.15)\n",
    "            y_range = (in_hand_t[1] - 0.15, in_hand_t[1] + 0.05)\n",
    "            z_range = (in_hand_t[2] - 0.05, in_hand_t[2] + 0.2)\n",
    "            print(f'inhand: x_range: {x_range} | y_range: {y_range} | z_range: {z_range}')\n",
    "            in_hand_pcd_baseframe_filtered = filter_pcd(in_hand_pcd_baseframe, x_range, y_range, z_range)\n",
    "            \n",
    "            # Transform to place pose\n",
    "            place_pose = demo_poses['placement_pose'][0]\n",
    "            in_hand_2_place_tf = np.matmul(place_pose, np.linalg.inv(in_hand_pose))\n",
    "            \n",
    "            in_hand_pcd_baseframe_filtered_h = np.hstack((in_hand_pcd_baseframe_filtered, np.ones((in_hand_pcd_baseframe_filtered.shape[0], 1))))\n",
    "            place_pcd_baseframe = np.matmul(in_hand_2_place_tf, in_hand_pcd_baseframe_filtered_h.T).T[:, :3]\n",
    "            \n",
    "            if i + inhand_idx == 0:\n",
    "                plot_multi_np([place_pcd_baseframe[::10], in_hand_pcd_baseframe_filtered[::10], scene_pcd_filtered[::10]])\n",
    "            \n",
    "            place_data = {\n",
    "                'scene_pcd': scene_pcd_filtered,\n",
    "                'in_hand_pcd': in_hand_pcd_baseframe_filtered,\n",
    "                'place_pcd': place_pcd_baseframe\n",
    "            }\n",
    "            \n",
    "            full_samples.append(place_data)\n",
    "    return full_samples\n",
    "taxpose_place_data = get_taxpose_place_data(\n",
    "    camera_pcds, \n",
    "    demo_poses, \n",
    "    T, \n",
    "    CAM2BASE_TF,\n",
    "    board_t_jitter=[0.1, 0.1, 0.0]\n",
    ")\n",
    "\n",
    "\n",
    "PCD_2_GRIPPER_IDX_MAP = {\n",
    "    0: 0,\n",
    "    1: 4,\n",
    "    2: 3,\n",
    "    3: 2,\n",
    "    4: 1\n",
    "}\n",
    "\n",
    "def get_taxpose_grasp_data(\n",
    "    camera_pcds: Dict[str, Dict[str, np.ndarray]], \n",
    "    demo_poses: Dict[str, np.ndarray], \n",
    "    T: List[np.ndarray],\n",
    "    CAM2BASE_TF: Dict[str, np.ndarray],\n",
    "    board_t: Tuple[float, float, float] = [0.24, 0, 0.15],\n",
    "    board_t_jitter: Tuple[float, float, float] = [0.0, 0.0, 0.0],\n",
    "    num_samples: int = 1,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \n",
    "    # Get the point cloud of the scene\n",
    "    out_of_way_pcds_world_tf = {}\n",
    "    for camera, data in camera_pcds.items():\n",
    "        if 'out_of_way' in data['pcds'].keys():\n",
    "            # TODO: For now assuming 1 scene pcd per camera\n",
    "            pcd = data['pcds']['out_of_way'][0] / 1000\n",
    "\n",
    "            o3d_pcd = o3d.geometry.PointCloud()\n",
    "            o3d_pcd.points = o3d.utility.Vector3dVector(pcd)\n",
    "\n",
    "            # o3d_pcd = o3d_pcd.voxel_down_sample(0.01)\n",
    "            np_pcd = np.asarray(o3d_pcd.points)\n",
    "\n",
    "            np_pcd_h = np.hstack((np_pcd, np.ones((np_pcd.shape[0], 1))))\n",
    "\n",
    "            np_pcd_tf = np.matmul(CAM2BASE_TF[camera], np_pcd_h.T).T[:, :3]\n",
    "\n",
    "            # TODO: For now assuming 1 scene pcd per camera\n",
    "            out_of_way_pcds_world_tf[camera] = [np_pcd_tf]\n",
    "            \n",
    "    # TODO: Add merging here\n",
    "    # For now just take CL8FC3100RL\n",
    "    # scene_pcd = np.concatenate([out_of_way_pcds_world_tf[cam][0] for cam in out_of_way_pcds_world_tf.keys()])\n",
    "    SCENE_CAMERA = 'CL8FC3100RL'\n",
    "    scene_pcd = out_of_way_pcds_world_tf[SCENE_CAMERA][0]\n",
    "    \n",
    "    GRIPPER_CAMERA = 'CL8FC3100NM'\n",
    "    \n",
    "    full_samples = []\n",
    "    for i in range(num_samples):\n",
    "        for gripper_idx in range(len(camera_pcds[GRIPPER_CAMERA]['pcds']['gripper'])):\n",
    "            board_jitter = [\n",
    "                np.random.uniform(-board_t_jitter[0], board_t_jitter[0]),\n",
    "                np.random.uniform(-board_t_jitter[1], board_t_jitter[1]),\n",
    "                np.random.uniform(-board_t_jitter[2], board_t_jitter[2])\n",
    "            ]\n",
    "            \n",
    "            # Filter the scene point cloud\n",
    "            x_range = (board_t[0] + board_jitter[0] - 0.2, board_t[0] + board_jitter[0] + 0.2)\n",
    "            y_range = (board_t[1] + board_jitter[1] - 0.15, board_t[1] + board_jitter[1] + 0.1)\n",
    "            z_range = (board_t[2] + board_jitter[2] - 0.05, board_t[2] + board_jitter[2] + 0.15)\n",
    "            print(f'scene: x_range: {x_range} | y_range: {y_range} | z_range: {z_range}')\n",
    "            scene_pcd_filtered = filter_pcd(scene_pcd, x_range, y_range, z_range)\n",
    "            # plot_multi_np([scene_pcd_filtered])\n",
    "            \n",
    "            # Get the point cloud of the gripper\n",
    "            # TODO: For now assuming 1 gripper pcd\n",
    "            assert 'gripper' in camera_pcds[GRIPPER_CAMERA]['pcds'].keys()\n",
    "            gripper_pcd = camera_pcds[GRIPPER_CAMERA]['pcds']['gripper'][gripper_idx] / 1000\n",
    "            \n",
    "            # Transform to robot base frame\n",
    "            cam2base_tf = CAM2BASE_TF[GRIPPER_CAMERA]\n",
    "            gripper_pcd_h = np.hstack((gripper_pcd, np.ones((gripper_pcd.shape[0], 1))))\n",
    "            gripper_pcd_baseframe = np.matmul(cam2base_tf, gripper_pcd_h.T).T[:, :3]\n",
    "            \n",
    "            gripper_close_up_files = glob.glob(\"/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/preprogrammed_poses/wp-grasp-place-0/gripper_close_up_pose_*.npz\")\n",
    "            print(f'gripper_close_up_files: {gripper_close_up_files}')\n",
    "            # Filter the point cloud\n",
    "            # TODO: For now assuming 1 in hand pcd\n",
    "            # TODO: Fix this: Need to do this mapping because poses were loaded out of order during demo collection\n",
    "            gripper_pose = demo_poses['gripper_close_up_pose'][PCD_2_GRIPPER_IDX_MAP[gripper_idx]]\n",
    "            \n",
    "            gripper_t = gripper_pose[:3, 3].copy()\n",
    "            eef_to_gripper_offset = np.array([0, -0.15, 0])\n",
    "            gripper_t += eef_to_gripper_offset\n",
    "            x_range = (gripper_t[0] - 0.15, gripper_t[0] + 0.15)\n",
    "            y_range = (gripper_t[1] - 0.1, gripper_t[1] + 0.1)\n",
    "            z_range = (gripper_t[2] - 0.1, gripper_t[2] + 0.25)\n",
    "            gripper_pcd_baseframe_filtered = filter_pcd(gripper_pcd_baseframe, x_range, y_range, z_range)\n",
    "            \n",
    "            # Transform to place pose\n",
    "            grasp_pose = demo_poses['grasp_pose'][0]\n",
    "            gripper_2_grasp_tf = np.matmul(grasp_pose, np.linalg.inv(gripper_pose))\n",
    "            \n",
    "            gripper_pcd_baseframe_filtered_h = np.hstack((gripper_pcd_baseframe_filtered, np.ones((gripper_pcd_baseframe_filtered.shape[0], 1))))\n",
    "            grasp_pcd_baseframe = np.matmul(gripper_2_grasp_tf, gripper_pcd_baseframe_filtered_h.T).T[:, :3]\n",
    "            \n",
    "            if i + gripper_idx == 0:\n",
    "                plot_multi_np([grasp_pcd_baseframe[::10], gripper_pcd_baseframe_filtered[::10], scene_pcd_filtered[::10]])\n",
    "            \n",
    "            grasp_data = {\n",
    "                'scene_pcd': scene_pcd_filtered,\n",
    "                'gripper_pcd': gripper_pcd_baseframe_filtered,\n",
    "                'grasp_pcd': grasp_pcd_baseframe\n",
    "            }\n",
    "            \n",
    "            full_samples.append(grasp_data)\n",
    "    \n",
    "    return full_samples\n",
    "taxpose_grasp_data = get_taxpose_grasp_data(\n",
    "    camera_pcds, \n",
    "    demo_poses, \n",
    "    T, \n",
    "    CAM2BASE_TF,\n",
    "    board_t_jitter=[0.05, 0.05, 0.0]    \n",
    ")\n",
    "\n",
    "\n",
    "def save_taxpose_data(data: Dict[str, np.ndarray], save_dir: str, demo_type: str, data_type: str = 'train', idx: int = 0) -> None:\n",
    "    if demo_type == 'place':\n",
    "        scene_pcd = data['scene_pcd']\n",
    "        in_hand_pcd = data['in_hand_pcd']\n",
    "        place_pcd = data['place_pcd']\n",
    "        clouds = np.concatenate([place_pcd, scene_pcd], axis=0)\n",
    "        print(f'clouds: {clouds.shape}')\n",
    "        classes = np.concatenate([np.zeros((place_pcd.shape[0])), np.ones((scene_pcd.shape[0]))], axis=0)\n",
    "        print(f'classes: {classes.shape}')\n",
    "        save_file_name = f'{idx}_teleport_obj_points.npz'\n",
    "    elif demo_type == 'grasp':\n",
    "        scene_pcd = data['scene_pcd']\n",
    "        gripper_pcd = data['gripper_pcd']\n",
    "        grasp_pcd = data['grasp_pcd']\n",
    "        clouds = np.concatenate([grasp_pcd, scene_pcd], axis=0)\n",
    "        print(f'clouds: {clouds.shape}')\n",
    "        classes = np.concatenate([np.full((grasp_pcd.shape[0]), 2), np.zeros((scene_pcd.shape[0]))], axis=0)\n",
    "        print(f'classes: {classes.shape}')\n",
    "        save_file_name = f'{idx}_pre_grasp_obj_points.npz'\n",
    "    \n",
    "    os.makedirs(f'{save_dir}/{demo_type}/{data_type}', exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        f'{save_dir}/{demo_type}/{data_type}/{save_file_name}', \n",
    "        clouds=clouds,\n",
    "        classes=classes,\n",
    "        colors=None,\n",
    "        shapenet_id=None\n",
    "    )\n",
    "\n",
    "save_dir = '/home/mfi/repos/rtc_vision_toolbox/data/train_data/taxpose/wp-grasp-place'\n",
    "# save_taxpose_data(\n",
    "#     taxpose_place_data, \n",
    "#     save_dir, \n",
    "#     demo_type='place', \n",
    "#     data_type='train',\n",
    "#     idx=0\n",
    "# )\n",
    "# save_taxpose_data(\n",
    "#     taxpose_grasp_data, \n",
    "#     save_dir, \n",
    "#     demo_type='grasp', \n",
    "#     data_type='train',\n",
    "#     idx=0\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
