{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camera.zed_ros.zed_ros import ZedRos\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import open3d as o3d\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "import copy\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.core.global_hydra import GlobalHydra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_np(plist):\n",
    "    \"\"\"\n",
    "    Plots multiple point clouds in the same plot using plotly\n",
    "    Assumes points are in meters coordinates.\n",
    "    \n",
    "    Args: \n",
    "        plist: list of np arrays of shape (N, 3)\n",
    "    \"\"\"\n",
    "    colors = [\n",
    "        '#1f77b4',  # muted blue\n",
    "        '#ff7f0e',  # safety orange\n",
    "        '#2ca02c',  # cooked asparagus green\n",
    "        '#d62728',  # brick red\n",
    "        '#9467bd',  # muted purple\n",
    "    ]\n",
    "    skip = 1\n",
    "    go_data = []\n",
    "    for i in range(len(plist)):\n",
    "        p_dp = plist[i]\n",
    "        plot = go.Scatter3d(x=p_dp[::skip,0], y=p_dp[::skip,1], z=p_dp[::skip,2], \n",
    "                     mode='markers', marker=dict(size=2, color=colors[i],\n",
    "                     symbol='circle'))\n",
    "        go_data.append(plot)\n",
    " \n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            aspectmode='data',\n",
    "        ),\n",
    "        height=1200,\n",
    "        width=1200,\n",
    "    )\n",
    "    fig = go.Figure(data=go_data, layout=layout)\n",
    "    \n",
    "    colors = ['red', 'green', 'blue']  # X, Y, Z axis colors\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    T = []\n",
    "    T.append(np.eye(4))\n",
    "\n",
    "    for tf in T:\n",
    "        origin = tf[:3, 3]\n",
    "        axes = tf[:3, :3]\n",
    "\n",
    "        for i in range(3):\n",
    "            axis_end = origin + 0.3*axes[:, i]\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=[origin[0], axis_end[0]],\n",
    "                y=[origin[1], axis_end[1]],\n",
    "                z=[origin[2], axis_end[2]],\n",
    "                mode='lines',\n",
    "                line=dict(color=colors[i], width=4),\n",
    "                name='Axis ' + str(i+1)\n",
    "            ))\n",
    "\n",
    "    for plot in go_data:\n",
    "        fig.add_trace(plot)\n",
    "    \n",
    "    # add axis lines and camera view\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis=dict(title='X'),\n",
    "        yaxis=dict(title='Y'),\n",
    "        zaxis=dict(title='Z'),\n",
    "        camera = dict(\n",
    "                      eye=dict(x=-1.30, y=0, z=-.25),\n",
    "                      center=dict(x=0., y=0, z=-0.25),\n",
    "                      up=dict(x=0, y=0, z=1),\n",
    "                     )\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1200,\n",
    "    )    \n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform points using a TF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pcd(pcd, tf=None, tf_filepath=None):\n",
    "    \"\"\"\n",
    "    Transforms a point cloud using a transformation matrix\n",
    "    \n",
    "    Args:\n",
    "        pcd: open3d.geometry.PointCloud\n",
    "        tf: 4x4 numpy array\n",
    "        tf_filepath: str, path to a .npy file containing a 4x4 transformation matrix\n",
    "        \n",
    "    Returns:\n",
    "        open3d.geometry.PointCloud\n",
    "    \"\"\"\n",
    "    if tf_filepath is not None:\n",
    "        tf = np.load(tf_filepath)\n",
    "        \n",
    "    if tf is not None:\n",
    "        pcd.transform(tf)\n",
    "        \n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table boundaries\n",
    "table_boundaries = {\n",
    "    'min': np.array([-0.5, -2, -1]),\n",
    "    'max': np.array([1.0, 2, 5])\n",
    "}\n",
    "\n",
    "# board boundaries\n",
    "board_boundaries = {\n",
    "    'min': np.array([0.0, -2, 0.15]),\n",
    "    'max': np.array([0.5, -0.0, 0.3])\n",
    "}\n",
    "\n",
    "# gripper boundaries\n",
    "gripper_boundaries = {\n",
    "    'min': np.array([0.2, -0.5, 0.3]),\n",
    "    'max': np.array([0.35, -0.05, .5])\n",
    "}\n",
    "\n",
    "ih_camera_focus = {\n",
    "    'min': np.array([-0.053, -0.028, 0.0]),\n",
    "    'max': np.array([0.053, 0.025, 0.3])\n",
    "}\n",
    "\n",
    "gripper_focus = {\n",
    "    'min': np.array([-0.2, -0.06, 0.0]),\n",
    "    'max': np.array([0.2, 0.06, 0.20]),\n",
    "\n",
    "}\n",
    "\n",
    "boundaries = {\n",
    "    'table': table_boundaries,\n",
    "    'board': board_boundaries,\n",
    "    'gripper': gripper_boundaries,\n",
    "    'ih_camera_focus': ih_camera_focus,\n",
    "    'gripper_focus': gripper_focus\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_points(points, boundary: str):\n",
    "    \"\"\"\n",
    "    Crops a point cloud based on a boundary\n",
    "    Assmes points are in meters coordinates.\n",
    "    \n",
    "    Args:\n",
    "        points:     np array of shape (N, 3)\n",
    "        boundary:   str, key in boundaries dict. \n",
    "                    Predefined boundaries are 'table', 'board', 'gripper'\n",
    "                    For custom boundaries, add to `boundaries` dict\n",
    "        \n",
    "    Returns:\n",
    "        np array of shape (M, 3)\n",
    "    \"\"\"\n",
    "    if boundary in boundaries:\n",
    "        world_boundaries = boundaries[boundary]\n",
    "    else:\n",
    "        print('Bounds not found')\n",
    "        return\n",
    "\n",
    "    #deep copy points\n",
    "    tmp_points = np.copy(points)\n",
    "    \n",
    "    tmp_points[np.isinf(tmp_points)] = np.nan\n",
    "    tmp_points[np.where(tmp_points[:, 0] < world_boundaries['min'][0])] = np.nan\n",
    "    tmp_points[np.where(tmp_points[:, 1] < world_boundaries['min'][1])] = np.nan\n",
    "    tmp_points[np.where(tmp_points[:, 2] < world_boundaries['min'][2])] = np.nan\n",
    "    tmp_points[np.where(tmp_points[:, 0] > world_boundaries['max'][0])] = np.nan\n",
    "    tmp_points[np.where(tmp_points[:, 1] > world_boundaries['max'][1])] = np.nan\n",
    "    tmp_points[np.where(tmp_points[:, 2] > world_boundaries['max'][2])] = np.nan\n",
    "    tmp_points = tmp_points[np.where(~np.isnan(tmp_points).any(axis=1))]\n",
    "    \n",
    "    return tmp_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Pointcloud(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.017 -1.    -0.026 -0.055]\n",
      " [ 1.    -0.017 -0.003 -0.012]\n",
      " [ 0.003 -0.026  1.     0.043]\n",
      " [ 0.     0.     0.     1.   ]] \n",
      "\n",
      "[[-0.017 -1.    -0.026 -0.07 ]\n",
      " [ 1.    -0.017 -0.003 -0.012]\n",
      " [ 0.003 -0.026  1.     0.074]\n",
      " [ 0.     0.     0.     1.   ]]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/mfi/repos/rtc_vision_toolbox/data/calibration_data/rs/T_eef2camera.npy\"\n",
    "T_test = np.load(file_path)\n",
    "print(np.round(T_test, 3),'\\n')\n",
    "T_test[0, 3] = T_test[0, 3] - 0.015\n",
    "T_test[2,3] = T_test[2,3] + 0.031\n",
    "print(np.round(T_test, 3))\n",
    "np.save(\"/home/mfi/repos/rtc_vision_toolbox/data/demonstrations/08-02-wp/calib_data/T_eef2cam3_new.npy\", T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_point_cloud(points, crop_box) -> o3d.geometry.PointCloud:\n",
    "\n",
    "    mask = np.logical_and.reduce(\n",
    "        (points[:, 0] >= crop_box['min'][0],\n",
    "            points[:, 0] <= crop_box['max'][0],\n",
    "            points[:, 1] >= crop_box['min'][1],\n",
    "            points[:, 1] <= crop_box['max'][1],\n",
    "            points[:, 2] >= crop_box['min'][2],\n",
    "            points[:, 2] <= crop_box['max'][2])\n",
    "    )\n",
    "    points = points[mask]\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(cfg) -> None:\n",
    "\n",
    "    project_dir = \"/home/mfi/repos/rtc_vision_toolbox/\"\n",
    "    data_dir = os.path.join(project_dir, cfg.data_dir)\n",
    "    num_demos = 1\n",
    "    # num_demos = cfg.training.num_demos\n",
    "    train_demos = np.ceil(\n",
    "        num_demos * (1-cfg.training.test_ratio)).astype(int)\n",
    "    test_demos = num_demos - train_demos\n",
    "    action_class = cfg.training.action.class_idx\n",
    "    anchor_class = cfg.training.anchor.class_idx\n",
    "    \n",
    "    teach_pcd_dir = os.path.join(data_dir, \"teach_data/pcd_data\")\n",
    "    teach_pose_dir = os.path.join(data_dir, \"teach_data/pose_data\")\n",
    "    train_save_dir = os.path.join(data_dir, \"learn_data/train\")\n",
    "    test_save_dir = os.path.join(data_dir, \"learn_data/test\")\n",
    "\n",
    "    # anchor point clouds with global coord.\n",
    "    anchor_points_list = []\n",
    "    action_points_list = []\n",
    "    \n",
    "    # for each demo:\n",
    "    for demo in range(num_demos):\n",
    "          \n",
    "        anchor_pcd_file = os.path.join(teach_pcd_dir, f\"demo{demo}_{cfg.training.anchor.view}0_{cfg.training.anchor.camera}_pointcloud.ply\")\n",
    "        anchor_pcd = o3d.io.read_point_cloud(anchor_pcd_file)\n",
    "        anchor_pcd, _ = anchor_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)  \n",
    "        anchor_pcd = anchor_pcd.uniform_down_sample(2)\n",
    "        \n",
    "        #scale point cloud\n",
    "        anchor_points = np.asarray(anchor_pcd.points) / 1000\n",
    "        anchor_pcd.points = o3d.utility.Vector3dVector(anchor_points)\n",
    "        \n",
    "        anchor_view_pose_file = os.path.join(teach_pose_dir, f\"demo{demo}_{cfg.training.anchor.view}0_pose.npy\")\n",
    "        T_eef2cam_file = cfg.devices.cameras[cfg.training.anchor.camera].setup.T_eef2cam\n",
    "        \n",
    "        # anchor view point\n",
    "        T_base2eef = np.load(anchor_view_pose_file)\n",
    "        T_eef2cam = np.load(os.path.join(project_dir, T_eef2cam_file))\n",
    "        T_base2cam = np.dot(T_base2eef, T_eef2cam)\n",
    "        \n",
    "        # target pose\n",
    "        T_base2targeteef = np.load(os.path.join(teach_pose_dir, f\"demo{demo}_placement_pose.npy\"))\n",
    "        T_ee2target = [\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 1, 0, 0],\n",
    "            [0, 0, 1, 0.212],\n",
    "            [0, 0, 0, 1]\n",
    "        ]        \n",
    "        T_base2target = np.dot(T_base2targeteef, T_ee2target)\n",
    "\n",
    "        # transform anchor pcd in target pose frame for easy cropping\n",
    "        anchor_tf = np.dot(np.linalg.inv(T_base2target), T_base2cam)\n",
    "        anchor_pcd = transform_pcd(anchor_pcd, tf=anchor_tf)\n",
    "\n",
    "        #crop_points\n",
    "        anchor_bounds = cfg.training.anchor.object_bounds\n",
    "        anchor_points = np.asarray(anchor_pcd.points)\n",
    "        object_bounds = {\n",
    "            'min': np.array(anchor_bounds.min)/1000,\n",
    "            'max': np.array(anchor_bounds.max)/1000\n",
    "        }\n",
    "        anchor_points = crop_point_cloud(anchor_points, object_bounds)\n",
    "        anchor_points_list.append(anchor_points)\n",
    "        \n",
    "        for var in range(cfg.training.action.view_variations.count):\n",
    "            # action point clouds @ target\n",
    "            action_pcd_file = os.path.join(teach_pcd_dir, f\"demo{demo}_{cfg.training.action.view}{var}_{cfg.training.action.camera}_pointcloud.ply\")\n",
    "            action_pcd = o3d.io.read_point_cloud(action_pcd_file)\n",
    "            action_pcd, _ = action_pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "            action_pcd = action_pcd.uniform_down_sample(2)\n",
    "\n",
    "            #scale point cloud\n",
    "            action_points = np.asarray(action_pcd.points) / 1000\n",
    "            action_pcd.points = o3d.utility.Vector3dVector(action_points)\n",
    "            \n",
    "            action_view_pose_file = os.path.join(teach_pose_dir, f\"demo{demo}_{cfg.training.action.view}{var}_pose.npy\")\n",
    "            T_base2action = np.load(action_view_pose_file)\n",
    "            T_base2cam_file = cfg.devices.cameras[cfg.training.action.camera].setup.T_base2cam\n",
    "            T_base2cam = np.load(os.path.join(project_dir, T_base2cam_file))\n",
    "            action_tf = np.linalg.inv(T_ee2target) @ (np.linalg.inv(T_base2action) @ T_base2cam)\n",
    "            action_pcd = transform_pcd(action_pcd, tf=action_tf)\n",
    "            \n",
    "            action_points = np.asarray(action_pcd.points)\n",
    "            action_points = action_points[action_points[:, 2] > 0]     \n",
    "            action_points_list.append(action_points)\n",
    "        \n",
    "        # save data to train or test dir\n",
    "    \n",
    "    return anchor_points_list, action_points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"../data/demonstrations/08-02-wp/\", version_base=\"1.3\")\n",
    "config: DictConfig = hydra.compose('place_object.yaml')\n",
    "\n",
    "anchor_points_list, action_points_list = prepare_dataset(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n"
     ]
    }
   ],
   "source": [
    "print(len(anchor_points_list), len(action_points_list))\n",
    "\n",
    "# plot_multi_np([anchor_points_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi_np([action_points_list[4], anchor_points_list[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
